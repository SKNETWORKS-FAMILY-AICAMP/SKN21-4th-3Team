"""
FileName    : embed_to_vectordb.py
Auth        : ë°•ìˆ˜ë¹ˆ
Date        : 2026-01-06
Description : ì „ì²˜ë¦¬ëœ JSONL ë°ì´í„°ë¥¼ ChromaDBì— ì„ë² ë”©í•˜ì—¬ ì €ì¥
Issue/Note  : docs_for_vectordb.jsonl â†’ ChromaDB ë²¡í„° ìŠ¤í† ì–´
"""

# -------------------------------------------------------------
# Imports
# -------------------------------------------------------------

import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.database import VectorStore

# -------------------------------------------------------------
# Configuration
# -------------------------------------------------------------

DEFAULT_INPUT_PATH = Path(__file__).parent.parent.parent / "data" / "processed" / "docs_for_vectordb.jsonl"
BATCH_SIZE = 100  # í•œ ë²ˆì— ì €ì¥í•  ë¬¸ì„œ ìˆ˜


# -------------------------------------------------------------
# Metadata Flatten Function
# -------------------------------------------------------------

def flatten_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì¤‘ì²©ëœ metadataë¥¼ ChromaDBì— ì €ì¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ í‰íƒ„í™”
    
    ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ê°’ìœ¼ë¡œ str, int, float, boolë§Œ ì§€ì›
    """
    flat = {}
    
    for key, value in metadata.items():
        if isinstance(value, dict):
            # ì¤‘ì²©ëœ dictëŠ” ì£¼ìš” í•„ë“œë§Œ ì¶”ì¶œ
            for sub_key, sub_value in value.items():
                if isinstance(sub_value, (str, int, float, bool)):
                    flat[f"{key}_{sub_key}"] = sub_value
        elif isinstance(value, (str, int, float, bool)):
            flat[key] = value
        elif value is None:
            flat[key] = ""  # None â†’ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
        else:
            flat[key] = str(value)  # ê¸°íƒ€ â†’ ë¬¸ìì—´ë¡œ ë³€í™˜
    
    return flat


# -------------------------------------------------------------
# Load and Embed Functions
# -------------------------------------------------------------

def load_jsonl(path: Path) -> List[Dict[str, Any]]:
    """JSONL íŒŒì¼ ë¡œë“œ"""
    docs = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                docs.append(json.loads(line))
    return docs


def embed_documents(
    input_path: Path = DEFAULT_INPUT_PATH,
    batch_size: int = BATCH_SIZE,
    limit: int = None
) -> Dict[str, int]:
    """
    JSONL íŒŒì¼ì˜ ë¬¸ì„œë“¤ì„ ChromaDBì— ì„ë² ë”©
    
    Args:
        input_path: ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ
        batch_size: ë°°ì¹˜ í¬ê¸°
        limit: ì„ë² ë”©í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
    
    Returns:
        {'total': 368944, 'embedded': 368944, 'skipped': 0}
    """
    print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {input_path}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("ğŸ“– ë°ì´í„° ë¡œë“œ ì¤‘...")
    docs = load_jsonl(input_path)
    total = len(docs)
    
    if limit:
        docs = docs[:limit]
        print(f"   (í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {limit}ê°œë§Œ ì²˜ë¦¬)")
    
    print(f"   ì „ì²´ ë¬¸ì„œ ìˆ˜: {total:,}")
    
    # 2. VectorStore ì´ˆê¸°í™”
    print("\nğŸ”§ VectorStore ì´ˆê¸°í™”...")
    vs = VectorStore()
    initial_count = vs.get_document_count()
    print(f"   í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ: {initial_count:,}")
    
    # 3. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©
    print(f"\nğŸš€ ì„ë² ë”© ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})...")
    
    stats = {'total': len(docs), 'embedded': 0, 'skipped': 0}
    
    for i in tqdm(range(0, len(docs), batch_size), desc="ì„ë² ë”© ì§„í–‰"):
        batch = docs[i:i + batch_size]
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, ID ì¶”ì¶œ
        texts = []
        metadatas = []
        doc_ids = []
        
        for idx, doc in enumerate(batch):
            global_idx = i + idx
            texts.append(doc['text'])
            
            # ë©”íƒ€ë°ì´í„° í‰íƒ„í™”
            flat_meta = flatten_metadata(doc.get('metadata', {}))
            metadatas.append(flat_meta)
            
            # ê³ ìœ  ID ìƒì„± (session_id + turn_index)
            session_id = flat_meta.get('session_id', f'doc_{global_idx}')
            turn_index = flat_meta.get('turn_index', global_idx)
            doc_id = f"{session_id}_turn_{turn_index}"
            doc_ids.append(doc_id)
        
        # VectorStoreì— ì¶”ê°€
        try:
            new_ids = vs.add_documents(
                documents=texts, 
                metadatas=metadatas,
                ids=doc_ids
            )
            stats['embedded'] += len(new_ids)
            stats['skipped'] += len(batch) - len(new_ids)
        except Exception as e:
            print(f"\nâš ï¸ ë°°ì¹˜ {i//batch_size + 1} ì—ëŸ¬: {e}")
            stats['skipped'] += len(batch)
    
    # 4. ê²°ê³¼ ì¶œë ¥
    final_count = vs.get_document_count()
    print(f"\nâœ… ì„ë² ë”© ì™„ë£Œ!")
    print(f"   - ì²˜ë¦¬ ìš”ì²­: {stats['total']:,}")
    print(f"   - ìƒˆë¡œ ì¶”ê°€: {stats['embedded']:,}")
    print(f"   - ìŠ¤í‚µ (ì¤‘ë³µ): {stats['skipped']:,}")
    print(f"   - ìµœì¢… ë¬¸ì„œ ìˆ˜: {final_count:,}")
    
    return stats


# -------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="JSONL ë°ì´í„°ë¥¼ VectorDBì— ì„ë² ë”©")
    parser.add_argument(
        "--input", 
        type=str, 
        default=str(DEFAULT_INPUT_PATH),
        help="ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=BATCH_SIZE,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 100)"
    )
    parser.add_argument(
        "--limit", 
        type=int, 
        default=None,
        help="í…ŒìŠ¤íŠ¸ìš© ìµœëŒ€ ë¬¸ì„œ ìˆ˜"
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        sys.exit(1)
    
    embed_documents(
        input_path=input_path,
        batch_size=args.batch_size,
        limit=args.limit
    )
