"""
FileName    : chain.py
Auth        : ìš°ì¬í˜„
Date        : 2026-01-06
Description : RAG ì „ì²´ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
Issue/Note  : DB ì—°ê²°, Rewrite, Retrieve, Answer ëª¨ë“  ë‹¨ê³„ í†µí•©
"""

# -------------------------------------------------------------
# Imports
# -------------------------------------------------------------

import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

# Root ê²½ë¡œ ì„¤ì •
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from config.model_config import create_chat_model
from src.database.db_manager import DatabaseManager
from src.database.vector_store import VectorStore
from src.rag.retriever import create_retriever, load_vector_db
from src.rag.rewrite import create_rewrite_chain, format_history
from src.rag.answer import create_answer_chain, format_sources
from src.rag.intent_router import route_query, QueryIntent, should_use_rag

# -------------------------------------------------------------
# RAG Main Class
# -------------------------------------------------------------

class RAGChain:
    """
    RAG ì‹œìŠ¤í…œì˜ ì „ì²´ íë¦„ì„ ì œì–´í•˜ëŠ” í´ë˜ìŠ¤ (LCEL ê¸°ë°˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±)
    """
    
    def __init__(self, db_manager: DatabaseManager = None):
        """
        ì´ˆê¸°í™” ë° ì²´ì¸ êµ¬ì„±
        """
        # 1. DB Manager
        self.db = db_manager if db_manager else DatabaseManager()
        
        # 2. Vector DB ë¡œë“œ
        self.vector_db = load_vector_db()
        
        # 3. ëª¨ë¸ ë° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model = create_chat_model()
        retriever_func = create_retriever(self.vector_db)
        
        # 4. ì„œë¸Œ ì²´ì¸ ì •ì˜
        rewrite_chain = create_rewrite_chain(self.model)
        answer_chain = create_answer_chain(self.model)
        
        # ---------------------------------------------------------
        # RAG ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„± (Compose Full RAG Pipeline)
        # Input: {"query": str, "history_text": str}
        # ---------------------------------------------------------
        
        # 1. ì§ˆë¬¸ ì¬ì‘ì„± (Rewrite)
        # Input: {query, history_text} -> Output: rewritten_query (str)
        rewrite_step = RunnablePassthrough.assign(
            rewritten_query=lambda x: rewrite_chain.invoke({
                "history": x["history_text"], 
                "query": x["query"]
            }).strip().strip('"\'').splitlines()[0]
        )
        
        # 2. ë¬¸ì„œ ê²€ìƒ‰ (Retrieve) & Context í¬ë§·íŒ…
        # Input: {..., rewritten_query} -> Output: source_docs (List), context (Str)
        def retrieve_and_format(x):
            docs = retriever_func(query=x["rewritten_query"])
            
            is_low_similarity = False
            
            # [ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ ê²½ê³ ]
            # ìœ ì‚¬ë„(Similarity) <= 0.65 ì¸ ê²½ìš° ê²½ê³  (Distance >= 0.65)
            if docs:
                first_dist = docs[0].get("distance")
                if first_dist is not None and first_dist >= 0.65:
                    print("[Warning] ë°ì´í„° ë‚´ì— ìœ ì‚¬í•œ ì •ë³´ê°€ ì—†ì–´ì„œ ì„ì˜ì˜ ë‚´ìš©ì„ ì¶œë ¥ ì¤‘ì…ë‹ˆë‹¤.")
                    is_low_similarity = True
            else:
                 print("[Warning] ë°ì´í„° ë‚´ì— ìœ ì‚¬í•œ ì •ë³´ê°€ ì—†ì–´ì„œ ì„ì˜ì˜ ë‚´ìš©ì„ ì¶œë ¥ ì¤‘ì…ë‹ˆë‹¤.")
                 is_low_similarity = True

            for i, doc in enumerate(docs):
                dist = doc.get("distance")
                if dist is not None:
                    print(f"[Retrieval] Doc {i+1} distance: {dist:.4f}")
                else:
                    print(f"[Retrieval] Doc {i+1} distance: None")
            return {"source_docs": docs, "context": format_sources(docs), "is_low_similarity": is_low_similarity}

        retrieve_step = RunnablePassthrough.assign(
            **{
                "data": lambda x: retrieve_and_format(x)
            }
        ) | RunnablePassthrough.assign(
            source_docs=lambda x: x["data"]["source_docs"],
            context=lambda x: x["data"]["context"],
            is_low_similarity=lambda x: x["data"]["is_low_similarity"]
        )
        
        # 3. ë‹µë³€ ìƒì„± (Answer)
        # Input: {..., context, history_text, rewritten_query} -> Output: answer (ìµœì¢… ë‹µë³€)
        def conditional_answer(x):
            if x.get("is_low_similarity", False):
                return "í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
            return answer_chain.invoke({
                "context": x["context"],
                "history": x["history_text"],
                "query": x["rewritten_query"] 
            })
            
        answer_step = RunnablePassthrough.assign(
            answer=lambda x: conditional_answer(x)
        )
        
        # Pipeline Chain
        self.rag_pipeline = rewrite_step | retrieve_step | answer_step

    def run(self, user_id: int, session_id: int, query: str) -> str:
        """
        ì‚¬ìš©ì ë°œí™”ì— ëŒ€í•œ RAG ì‘ë‹µ ìƒì„± ë° ì²˜ë¦¬ ì „ì²´ ê³¼ì •
        [2026-01-28] Intent Router í†µí•© - ì˜ë„ì— ë”°ë¼ RAG ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
        """
        print(f"\n[Flow Start] User: {user_id}, Session: {session_id}")
        
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        self.db.add_chat_message(session_id, "user", query)
        
        print(f"[Step] Input: {query}")
        
        start_time = time.time()
        
        try:
            # ============================================
            # 2. Intent Router: ì˜ë„ ë¶„ë¥˜
            # ============================================
            intent, direct_response, needs_rag = route_query(query, self.model)
            print(f"[Intent] {intent.value} | Needs RAG: {needs_rag}")
            
            # ============================================
            # 3-A. GREETING/CHITCHAT: RAG ì—†ì´ ì§ì ‘ ì‘ë‹µ
            # ============================================
            if not needs_rag and direct_response:
                # CLOSING ì¸ ê²½ìš° ìš”ì•½ ìƒì„± ì‹œë„ (ë§Œì•½ direct_responseê°€ ë‹¨ìˆœ ì¸ì‚¬ë¼ë©´ ë®ì–´ì“°ê¸° ë  ìˆ˜ ìˆìŒ, 
                # í•˜ì§€ë§Œ intent_routerì—ì„œ CLOSINGì€ direct_responseê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ. í™•ì¸ í•„ìš”. 
                # ì¼ë‹¨ CLOSINGì€ ë³„ë„ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬)
                pass

            if intent == QueryIntent.CLOSING:
                answer = self._generate_session_summary(session_id)
                self.db.add_chat_message(session_id, "assistant", answer)
                print(f"[Flow End] ìƒë‹´ ìš”ì•½ ë° ì¢…ë£Œ ì™„ë£Œ")
                return answer

            if not needs_rag and direct_response:
                answer = direct_response
                print(f"[Direct Response] Intent={intent.value}")
                
                # 5. Assistant ë©”ì‹œì§€ ì €ì¥
                self.db.add_chat_message(session_id, "assistant", answer)
                
                end_time = time.time()
                print(f"[System] Response Time: {end_time - start_time:.2f}s")
                print(f"[Flow End] ì§ì ‘ ì‘ë‹µ ì™„ë£Œ")
                return answer
            
            # ============================================
            # 3-B. CRISIS: ê¸´ê¸‰ ì‘ë‹µ + ì „ë¬¸ê°€ ì—°ê²°
            # ============================================
            if intent == QueryIntent.CRISIS:
                answer = direct_response if direct_response else "ì§€ê¸ˆ ë§ì´ í˜ë“œì‹œêµ°ìš”. ì „ë¬¸ ìƒë‹´ì‚¬ì™€ ì´ì•¼ê¸°í•´ ë³´ì‹œëŠ” ê²ƒì„ ê¶Œí•´ë“œë ¤ìš”. ğŸ“ ìì‚´ì˜ˆë°©ìƒë‹´ì „í™”: 1393 (24ì‹œê°„)"
                self._handle_expert_referral(session_id, answer)
                self.db.add_chat_message(session_id, "assistant", answer)
                
                end_time = time.time()
                print(f"[System] Response Time: {end_time - start_time:.2f}s")
                print(f"[Flow End] ìœ„ê¸° ëŒ€ì‘ ì™„ë£Œ")
                return answer
            
            # ============================================
            # 3-C. EMOTION/QUESTION: RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            # ============================================
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ
            history_objs = self.db.get_chat_history(session_id)
            history_dicts = [{"role": msg.role, "content": msg.content} for msg in history_objs]
            pre_history = history_dicts[:-1]
            history_text = format_history(pre_history)
            
            # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = self.rag_pipeline.invoke({
                "query": query,
                "history_text": history_text
            })
            
            answer = result["answer"].strip()
            
            # 4. ì „ë¬¸ê°€ ì—°ê²° ê°ì§€ (í›„ì²˜ë¦¬)
            if "[EXPERT_REFERRAL_NEEDED]" in answer:
                answer = answer.replace("[EXPERT_REFERRAL_NEEDED]", "").strip()
                self._handle_expert_referral(session_id, answer)
                if "ìƒë‹´" not in answer:
                    answer += "\n"
            
            # 5. Assistant ë©”ì‹œì§€ ì €ì¥
            self.db.add_chat_message(session_id, "assistant", answer)
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            print(f"[System] Response Time: {elapsed_time:.2f}s")
            
            print(f"[Flow End] RAG ì‘ë‹µ ì™„ë£Œ")
            return answer
            
        except Exception as e:
            print(f"[Error] RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def stream(self, user_id: int, session_id: int, query: str, debug: bool = False):
        """
        ì‚¬ìš©ì ë°œí™”ì— ëŒ€í•œ RAG ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        debug=True ì¼ ê²½ìš°: 
            yield {"type": "debug", "data": {...}}  # ê²€ìƒ‰ ê²°ê³¼ ë“±
            yield {"type": "content", "data": "..."} # ë‹µë³€ ì²­í¬
        debug=False ì¼ ê²½ìš°:
            yield "ë‹µë³€ ì²­í¬" ...
        """
        print(f"\n[Flow Start (Stream)] User: {user_id}, Session: {session_id}, Debug: {debug}")
        
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        self.db.add_chat_message(session_id, "user", query)
        
        try:
            # 2. Intent Router
            intent, direct_response, needs_rag = route_query(query, self.model)
            
            # 3-A. GREETING/CHITCHAT: ì§ì ‘ ì‘ë‹µ
            if not needs_rag and direct_response:
                # Debug Info Yield
                if debug:
                    yield {
                        "type": "debug",
                        "data": {
                            "intent": intent.value,
                            "rewritten_query": query,
                            "sources": [],
                            "context_length": 0,
                            "note": "RAG ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ì‘ë‹µ (Intent Router)"
                        }
                    }

                self.db.add_chat_message(session_id, "assistant", direct_response)
                # ìŠ¤íŠ¸ë¦¬ë° í‰ë‚´ (ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼)
                for i in range(0, len(direct_response), 3): # 3ê¸€ìì”© ëŠì–´ì„œ
                    chunk = direct_response[i:i+3]
                    if debug:
                        yield {"type": "content", "data": chunk}
                    else:
                        yield chunk
                    time.sleep(0.05)
                return

            # 3-B. CRISIS: ìœ„ê¸° ëŒ€ì‘
            if intent == QueryIntent.CRISIS:
                answer = direct_response if direct_response else "ì§€ê¸ˆ ë§ì´ í˜ë“œì‹œêµ°ìš”. ì „ë¬¸ ìƒë‹´ì‚¬ì™€ ì´ì•¼ê¸°í•´ ë³´ì‹œëŠ” ê²ƒì„ ê¶Œí•´ë“œë ¤ìš”. ğŸ“ ìì‚´ì˜ˆë°©ìƒë‹´ì „í™”: 1393 (24ì‹œê°„)"
                
                # Debug Info Yield
                if debug:
                    yield {
                        "type": "debug",
                        "data": {
                            "intent": intent.value,
                            "rewritten_query": query,
                            "sources": [],
                            "context_length": 0,
                            "note": "ìœ„ê¸° ìƒí™© - ì¦‰ì‹œ ì „ë¬¸ê°€ ì—°ê²°"
                        }
                    }

                self._handle_expert_referral(session_id, answer)
                self.db.add_chat_message(session_id, "assistant", answer)
                
                # ìŠ¤íŠ¸ë¦¬ë° í‰ë‚´
                for i in range(0, len(answer), 3):
                    chunk = answer[i:i+3]
                    if debug:
                        yield {"type": "content", "data": chunk}
                    else:
                        yield chunk
                    time.sleep(0.05)
                return
            
            # 3-C. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            
            # History Load
            history_objs = self.db.get_chat_history(session_id)
            history_dicts = [{"role": msg.role, "content": msg.content} for msg in history_objs]
            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì´ë¯¸ ì €ì¥í–ˆìœ¼ë¯€ë¡œ, ê·¸ ì´ì „ê¹Œì§€ë¥¼ íˆìŠ¤í† ë¦¬ë¡œ ì‚¬ìš©
            pre_history = history_dicts[:-1]
            history_text = format_history(pre_history)
            
            # Rewrite (Sync)
            rewrite_chain = create_rewrite_chain(self.model)
            rewritten_query = rewrite_chain.invoke({
                "history": history_text, 
                "query": query
            }).strip().strip('"\'').splitlines()[0]
            
            # Retrieve (Sync)
            retriever_func = create_retriever(self.vector_db)
            docs = retriever_func(query=rewritten_query)
            
            # [Relevance Filtering]
            SIMILARITY_THRESHOLD = 0.40 # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸(ko-sroberta) ê±°ë¦¬ ì²™ë„ì— ë§ì¶¤
            valid_docs = []
            
            # Debug Info Formatting & Yield
            if debug:
                debug_info_sources = []
                for i, doc in enumerate(docs[:5]):
                    meta = doc.get("metadata", {})
                    distance = round(doc.get("distance", meta.get("distance", 0)), 4)
                    
                    is_valid = distance <= SIMILARITY_THRESHOLD
                    if is_valid:
                        valid_docs.append(doc)
                    
                    window_text = meta.get("window_text", "") or ""
                    content = doc.get("content", "")
                    display_content = window_text if len(window_text) > len(content) else content
                    
                    status_prefix = "" if is_valid else "[SKIPPED-Low Relevance] "
                    
                    debug_info_sources.append({
                        "rank": i + 1,
                        "session_id": meta.get("session_id", "N/A"),
                        "category": meta.get("category", "N/A"),
                        "turn_idx": meta.get("turn_idx", "N/A"),
                        "content": status_prefix + f"ë‚´ë‹´ì: {display_content}\n[ìƒë‹´ì‚¬ ë‹µë³€]: {meta.get('counselor_response', '(ë‹µë³€ ì—†ìŒ)')[:100]}...",
                        "distance": distance
                    })

                yield {
                    "type": "debug",
                    "data": {
                        "intent": intent.value,
                        "rewritten_query": rewritten_query,
                        "sources": debug_info_sources,
                        "context_length": sum(len(d.get("page_content", "") or d.get("content", "")) for d in valid_docs),
                        "note": f"Threshold({SIMILARITY_THRESHOLD}) ì ìš©: {len(valid_docs)}/{len(docs)} ê±´ ì‚¬ìš©"
                    }
                }
                
                # Update docs to only valid ones for context generation
                docs = valid_docs
            else:
                # Debug mode ì•„ë‹ ë•Œë„ í•„í„°ë§ ì ìš©
                docs = [d for d in docs if d.get("distance", d.get("metadata", {}).get("distance", 0)) <= SIMILARITY_THRESHOLD]

            if not docs:
                context = "ê´€ë ¨ëœ ìƒë‹´ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤. (ìœ„ë¡œì™€ ê³µê°, ì¼ë°˜ì ì¸ ì‹¬ë¦¬í•™ ì§€ì‹ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”)"
            else:
                context = format_sources(docs)
            
            # ìœ ì‚¬ë„ ì²´í¬ (ì˜µì…˜)
            if not docs:
                pass 
            
            # Answer Stream
            answer_chain = create_answer_chain(self.model)
            full_answer = ""
            
            for chunk in answer_chain.stream({
                "context": context,
                "history": history_text,
                "query": rewritten_query
            }):
                full_answer += chunk
                if debug:
                    yield {"type": "content", "data": chunk}
                else:
                    yield chunk
            
            # 4. ì „ë¬¸ê°€ ì—°ê²° ê°ì§€ ë° í›„ì²˜ë¦¬ (Logging Only)
            clean_answer = full_answer
            if "[EXPERT_REFERRAL_NEEDED]" in full_answer:
                clean_answer = full_answer.replace("[EXPERT_REFERRAL_NEEDED]", "").strip()
                self._handle_expert_referral(session_id, clean_answer)
                if "ìƒë‹´" not in clean_answer:
                    clean_answer += "\n"
            
            # 5. Assistant ë©”ì‹œì§€ ì €ì¥ (Cleaned version)
            self.db.add_chat_message(session_id, "assistant", clean_answer)
            print(f"[Flow End (Stream)] RAG ì‘ë‹µ ì™„ë£Œ")
            
        except Exception as e:
            print(f"[Error] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            err_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            if debug:
                yield {"type": "content", "data": err_msg}
            else:
                yield err_msg

    def run_with_debug(self, query: str, history: List[Dict[str, str]] = []) -> Dict[str, Any]:
        """
        [í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©] RAG íŒŒì´í”„ë¼ì¸ì˜ ì¤‘ê°„ ê²°ê³¼ê¹Œì§€ í¬í•¨í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” í”„ë¡¬í”„íŠ¸ìš© íˆìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸ [{"role": "user", "content": "..."}]
        
        Returns:
            Dict: {
                "input_query": str,
                "rewritten_query": str,
                "source_docs": List[Dict], # {content, metadata, distance}
                "context": str,
                "answer": str
            }
        """
        history_text = format_history(history)
        
        try:
            result = self.rag_pipeline.invoke({
                "query": query,
                "history_text": history_text
            })
            
            # í›„ì²˜ë¦¬ ì „ ìˆœìˆ˜ ë‹µë³€ ë°˜í™˜
            return {
                "input_query": query,
                "rewritten_query": result.get("rewritten_query", ""),
                "source_docs": result.get("source_docs", []),
                "context": result.get("context", ""),
                "answer": result.get("answer", "").strip()
            }
            
        except Exception as e:
            return {
                "error": str(e)
            }

    def _handle_expert_referral(self, session_id: int, answer: str):
        """ì „ë¬¸ê°€ ì—°ê²° DB ê¸°ë¡"""
        try:
            self.db.create_expert_referral(
                session_id=session_id,
                severity_level="high",
                recommended_action="ì „ë¬¸ ìƒë‹´ì‚¬ ì—°ê²° ê¶Œì¥"
            )
        except Exception as e:
            print(f"[Error] ì „ë¬¸ê°€ ì—°ê²° ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _generate_session_summary(self, session_id: int) -> str:
        """
        [2026-01-29] ìƒë‹´ ì¢…ë£Œ ì‹œ, ì´ë²ˆ ì„¸ì…˜ì—ì„œ ì œì•ˆëœ ì•ˆì •í™” ê¸°ë²•/ì¡°ì–¸ ìš”ì•½
        """
        history = self.db.get_chat_history(session_id)
        if not history:
            return "ì§„í–‰ëœ ìƒë‹´ ë‚´ì—­ì´ ì—†ì–´ ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”."

        conversation_text = ""
        for msg in history:
            if msg.role == "system": continue
            role = "ìƒë‹´ì‚¬" if msg.role == "assistant" else "ë‚´ë‹´ì"
            conversation_text += f"{role}: {msg.content}\n"

        summary_prompt = f"""
[ì—­í• ]
ë‹¹ì‹ ì€ ì‹¬ë¦¬ ìƒë‹´ ë‚´ìš©ì„ ì •ë¦¬í•´ì£¼ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.
ì•„ë˜ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, **ìƒë‹´ì‚¬ê°€ ë‚´ë‹´ìì—ê²Œ ì œì•ˆí–ˆë˜ ì‹¬ë¦¬ì  ì•ˆì •í™” ê¸°ë²•ì´ë‚˜ ì‹¤ì§ˆì ì¸ ì¡°ì–¸ë“¤**ì„ ìš”ì•½í•´ì„œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

[ëŒ€í™” ê¸°ë¡]
{conversation_text}

[ìš”ì•½ ê·œì¹™]
1. ìƒë‹´ì‚¬ê°€ ì œì•ˆí•œ **êµ¬ì²´ì ì¸ í•´ê²°ì±…, ê¸°ë²•(ì˜ˆ: í˜¸í¡ë²•, ì ìˆ˜ ë§¤ê¸°ê¸° ë“±), í–‰ë™ ì§€ì¹¨**ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
2. ë‹¨ìˆœíˆ "ê³µê°í•´ì£¼ì—ˆë‹¤" ê°™ì€ ë‚´ìš©ì€ ì ì§€ ë§ˆì„¸ìš”.
3. ë‚´ë‹´ìê°€ ì‹¤ì²œí•  ìˆ˜ ìˆë„ë¡ [ì˜¤ëŠ˜ì˜ ì‹¬ë¦¬ ì²˜ë°©] í˜•ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ë¦¬ìŠ¤íŠ¸ì—… í•´ì£¼ì„¸ìš”.
4. ë§ˆì§€ë§‰ì—ëŠ” ë”°ëœ»í•œ ê²©ë ¤ì˜ í•œ ë§ˆë””ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
[ì˜¤ëŠ˜ì˜ ì‹¬ë¦¬ ì²˜ë°©] ğŸ“
1. (ê¸°ë²• ì´ë¦„): (êµ¬ì²´ì  ë°©ë²• ìš”ì•½)
2. ...

(ë§ˆë¬´ë¦¬ ê²©ë ¤)
"""
        response = self.model.invoke(summary_prompt)
        return response.content if hasattr(response, 'content') else str(response)

# -------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------
if __name__ == "__main__":
    # Test Setup
    print("=== RAG Chain Test (LCEL) ===")
    
    # ì„ì‹œ DB Manager (í…ŒìŠ¤íŠ¸ìš©)
    test_db = DatabaseManager(echo=False)
    rag_chain = RAGChain(db_manager=test_db)
    
    # 1. User/Session Create
    try:
        user = test_db.create_user("test_lcel_user_01")
    except Exception:
        user = test_db.get_user_by_username("test_lcel_user_01")
        
    session = test_db.create_chat_session(user.id)
    
    # 2. Run Flow
    q1 = "ì‚¬ëŠ”ê²Œ ì¬ë¯¸ê°€ ì—†ì–´"
    ans1 = rag_chain.run(user.id, session.id, q1)
    print(f"\n[Bot]: {ans1}\n")

