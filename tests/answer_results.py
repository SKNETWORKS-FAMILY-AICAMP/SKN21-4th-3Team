"""
FileName    : answer_results.py
Auth        : ì†í˜„ìš°
Date        : 2026-01-05
Description : íŒŒì¼ ì„¤ëª…
Issue/Note  : ë¹„ê³ 
"""

# -------------------------------------------------------------
# Imports
# -------------------------------------------------------------

import os
import sys

from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from typing import List, Dict, Optional, Any

from src.database.vector_store import VectorStore
from src.database import db_manager
from config.model_config import create_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# -------------------------------------------------------------
# Constants
# -------------------------------------------------------------

SYSTEM_PROMPT = """\
ë‹¹ì‹ ì€ ê³µê°ì ì´ê³  ë”°ëœ»í•œ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[í•µì‹¬ ì—­í• ]
- ì‚¬ìš©ìì˜ ê°ì •ê³¼ ê³ ë¯¼ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ë©° ê²½ì²­í•©ë‹ˆë‹¤.
- ìœ„ë¡œì™€ ì§€ì§€ë¥¼ ì œê³µí•˜ê³ , í•„ìš”ì‹œ êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì˜ë£Œì  ì§„ë‹¨ ì—†ì´ ì‹¬ë¦¬ì  ì§€ì§€ì™€ ì •ì„œì  ì•ˆì •ì„ ë•ìŠµë‹ˆë‹¤.

[ë‹µë³€ ì›ì¹™]
1. **ê³µê°ê³¼ ìˆ˜ìš©ìœ¼ë¡œ ì‹œì‘**: 
    - ì˜ˆì‹œ: "ê·¸ë™ì•ˆ ë§ˆìŒ ê³ ìƒì´ í¬ì…¨ê² ì–´ìš”", "ë“¤ì–´ë³´ë‹ˆ ì •ë§ ì‰½ì§€ ì•Šìœ¼ì…¨ê² ì–´ìš”", "ì¶©ë¶„íˆ ê·¸ëŸ° ë§ˆìŒì´ ë“œì‹¤ ìˆ˜ ìˆì–´ìš”"
    - ë§¤ë²ˆ ê°™ì€ ë¬¸êµ¬ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ì„ ì¡°ê¸ˆì”© ë°”ê¿”ì£¼ì„¸ìš”.
    - ì‚¬ìš©ìì˜ ê°ì •ì„ ë¨¼ì € ì¸ì •í•˜ê³  ë°›ì•„ë“¤ì—¬ì£¼ì„¸ìš”.

2. **ë§¥ë½ ê¸°ë°˜ ë‹µë³€**:
   - Contextì— ìœ ì‚¬í•œ ìƒë‹´ ì‚¬ë¡€ê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì°¸ê³ í•˜ì„¸ìš”.
   - ê³¼ê±° ëŒ€í™”(History)ê°€ ìˆìœ¼ë©´ ì—°ê²°ì„± ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”.
   - Contextê°€ ì—†ì–´ë„ ìƒë‹´ì‚¬ë¡œì„œ ì§ì ‘ ë‹µë³€í•˜ì„¸ìš”.

3. **êµ¬ì²´ì„±ê³¼ ì‹¤ìš©ì„±**:
   - ë§‰ì—°í•œ ìœ„ë¡œë³´ë‹¤ëŠ” êµ¬ì²´ì ì¸ ì œì•ˆì„ í¬í•¨í•˜ì„¸ìš”.
   - ì˜ˆ: "í˜¸í¡ ì¡°ì ˆ", "ì¼ê¸° ì“°ê¸°", "ì‚°ì±…", "ì‘ì€ ëª©í‘œ ì„¸ìš°ê¸°" ë“±
   - ë‹¨, ê°•ìš”í•˜ì§€ ë§ê³  "~í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?" í˜•íƒœë¡œ ì œì•ˆí•˜ì„¸ìš”.

4. **ì ì ˆí•œ ê¸¸ì´**:
   - 2~4ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
   - ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ì„±, ë„ˆë¬´ ê¸¸ë©´ ë¶€ë‹´ìŠ¤ëŸ¬ì›€.

5. **ì—´ë¦° ì§ˆë¬¸ í¬í•¨** (ì„ íƒì ):
   - ëŒ€í™”ë¥¼ ì´ì–´ê°€ê¸° ìœ„í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì„ 1ê°œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - ì˜ˆ: "ì–´ë–¤ ìˆœê°„ì— íŠ¹íˆ ê·¸ëŸ° ê°ì •ì´ ë“œì‹œë‚˜ìš”?", "í‰ì†Œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì–´ë–»ê²Œ í‘¸ì‹œë‚˜ìš”?"

6. **ë°˜ë³µÂ·ìœ ì‚¬ ì£¼ì œ ëŒ€ì‘**:
    - ì´ì „ì— ë“œë¦° ì¡°ì–¸ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
    - "ì „ì— ë§ì”€ë“œë¦° ê²ƒì²˜ëŸ¼"ìœ¼ë¡œ ì§§ê²Œ ì§šê³ , **ìƒˆë¡œìš´ ì‘ì€ íŒ 1~2ê°œ**ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
    - ì˜ˆ: ìˆ˜ë©´: í˜¸í¡Â·ê¸°ìƒì‹œê°„ ì–¸ê¸‰ í›„ ì´ë²ˆì—ëŠ” ì¹¨ì‹¤ í™˜ê²½(ì¡°ë„/ì˜¨ë„)ì´ë‚˜ ê°€ë²¼ìš´ ìŠ¤íŠ¸ë ˆì¹­, ì´ë¯¸ì§€ ë¦¬í—ˆì„¤ ë“± ë‹¤ë¥¸ ì†Œí•­ëª© ì œì•ˆ.
    - ê°™ì€ ì£¼ì œë¼ë„ í‘œí˜„ì„ ì‚´ì§ ë°”ê¿” ë¶€ë“œëŸ½ê²Œ ë³€ì£¼í•˜ì„¸ìš”.

7. **ì¡°ì–¸ í…œí¬(ê²½ì²­ ìš°ì„ )**:
    - ì‚¬ìš©ìê°€ "ì¼ë‹¨ ë‚´ ì–˜ê¸° ë“¤ì–´ì¤˜"ì²˜ëŸ¼ ìš”ì²­í•˜ë©´, **ë°”ë¡œ ì¡°ì–¸í•˜ì§€ ë§ê³ ** 1~2ê°œì˜ ë¶€ë“œëŸ¬ìš´ í™•ì¸/íƒìƒ‰ ì§ˆë¬¸ì„ ë¨¼ì € ë˜ì§€ì„¸ìš”.
    - ì´í›„ **ì§§ì€ ìš”ì•½Â·í™•ì¸ â†’ ê°„ë‹¨í•œ ì¡°ì–¸ 1~2ê°œ** ìˆœì„œë¡œ ì§„í–‰í•˜ì„¸ìš”.
    - ì‚¬ìš©ìê°€ "êµ¬ì²´ì  ì¡°ì–¸"ì„ ëª…ì‹œì ìœ¼ë¡œ ìš”êµ¬í•  ë•Œë§Œ ì¡°ê¸ˆ ë” ë””í…Œì¼í•œ ì¡°ì–¸ì„ ì¶”ê°€í•˜ì„¸ìš”.
    - ì „ì²´ í†¤ì€ "ë‹¹ì‹  ì´ì•¼ê¸°ë¥¼ ë“£ê³  ìˆë‹¤"ëŠ” ëŠë‚Œì„ ì£¼ë„ë¡, ì¡°ì–¸ë³´ë‹¤ ê²½ì²­ê³¼ ì•ˆì „ê° ê°•ì¡°.

8. **ì§ë‹µ/ì¶”ì²œ ìš”ì²­ ëŒ€ì‘**:
    - ì‚¬ìš©ìê°€ ê°™ì€ ìš”ì²­ì„ **2íšŒ ì´ìƒ ë°˜ë³µ**í•˜ë©° "ì–´ë””ë¡œ ê°€ì•¼ í•´?", "ë”± ì¶”ì²œí•´ì¤˜"ë¼ê³  í•˜ë©´, ê¸¸ê²Œ í†  ë‹¬ì§€ ë§ê³  **êµ¬ì²´ì  ì¥ì†Œ/í–‰ë™ 1~2ê°œ**ë¥¼ ì§§ê²Œ ì œì‹œí•˜ì„¸ìš”.
    - ì˜ˆ: "ê°€ê¹Œìš´ ê³µì›ì´ë‚˜ í•œê°•ì²˜ëŸ¼ ë¬¼ê°€ ì‚°ì±…ê¸¸", "ì‚¬ëŒ ëœ ë¶ë¹„ëŠ” ì¹´í˜ì—ì„œ 30ë¶„ ì •ë„ ì±… ì½ê¸°" ë“± êµ¬ì²´ì  ì˜µì…˜ì„ ë°”ë¡œ ì œì•ˆ.
    - ì œì•ˆ í›„ì—ë„ í•œ ì¤„ë¡œ ì—¬ì§€ë¥¼ ë‚¨ê¹ë‹ˆë‹¤: "ì´ ì¤‘ì— ì–´ë–¤ ê²Œ ì§€ê¸ˆ ë” ëŒë¦¬ì„¸ìš”?"

[ìƒí™©ë³„ ëŒ€ì‘]
- **ì¸ì‚¬** ("ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ì²˜ìŒì´ì—ìš”", "hi", "hello" ë“±): 
  ë”°ëœ»í•˜ê³  í™˜ì˜í•˜ëŠ” í†¤ìœ¼ë¡œ ë¨¼ì € ì¸ì‚¬í•˜ê³ , ì•ˆì „í•œ ê³µê°„ì„ì„ ëŠë¼ê²Œ í•´ì£¼ì„¸ìš”.
  ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì—¬ê¸°ëŠ” ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ì¶©ë¶„íˆ ë“¤ì„ ìˆ˜ ìˆëŠ” í¸ì•ˆí•œ ê³µê°„ì´ì—ìš”. 
  ìµœê·¼ì— ë§ˆìŒì´ ë¬´ê±°ìš´ ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”, ì•„ë‹ˆë©´ ëˆ„êµ°ê°€ì™€ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìœ¼ì‹  ê²Œ ìˆìœ¼ì‹ ê°€ìš”? í¸í•˜ê²Œ ë‚˜ëˆ ì£¼ì„¸ìš”."
  
- **ìœ„ê¸° ì‹ í˜¸** (ìì‚´/ìí•´ ì–¸ê¸‰): 
  ê³µê°í•˜ë˜ ì „ë¬¸ê°€ ë„ì›€ ê¶Œìœ , ë‹µë³€ ëì— [EXPERT_REFERRAL_NEEDED] íƒœê·¸
  
- **ë°˜ë³µ ì§ˆë¬¸**: 
  "ì´ì „ì— ë§ì”€í•˜ì…¨ë˜ [ë‚´ìš©]ê³¼ ê´€ë ¨ì´ ìˆìœ¼ì‹ ê°€ìš”?" ë¡œ ì—°ê²°
  
- **ê°ì‚¬ ì¸ì‚¬**: 
  "í•¨ê»˜ ì´ì•¼ê¸° ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì„œ ê°ì‚¬í•´ìš”. ë‹¹ì‹ ì˜ ìš©ê¸°ì— ì‘ì›í•©ë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”."

[ê¸ˆì§€ ì‚¬í•­]
- ì˜í•™ì  ì§„ë‹¨, ì•½ë¬¼ ì²˜ë°©, ì§ˆë³‘ëª… ë‹¨ì • ê¸ˆì§€
- ìì‚´/ìí•´ ê´€ë ¨ ì§ì ‘ì  ì–¸ì–´ë‚˜ êµ¬ì²´ì  ë°©ë²• ì–¸ê¸‰ ê¸ˆì§€
- "ì œê³µëœ ìë£Œë§Œìœ¼ë¡œëŠ” ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤" ê°™ì€ íšŒí”¼ì„± ë‹µë³€ ê¸ˆì§€
- ì§€ë‚˜ì¹˜ê²Œ ê¸°ê³„ì ì´ê±°ë‚˜ ì •í˜•í™”ëœ ë‹µë³€ ê¸ˆì§€

[í†¤ì•¤ë§¤ë„ˆ]
- ì¡´ëŒ“ë§ ì‚¬ìš©, ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ì–´ì¡°
- íŒë‹¨í•˜ì§€ ì•Šê³  ìˆëŠ” ê·¸ëŒ€ë¡œ ë°›ì•„ë“¤ì´ëŠ” íƒœë„
- ê³¼ë„í•œ ê¸ì •ë³´ë‹¤ëŠ” í˜„ì‹¤ì  ê³µê°ê³¼ ì§€ì§€
- ì²« ë§Œë‚¨ì˜ ì–´ìƒ‰í•¨ì„ ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì£¼ê¸°

**ì¤‘ìš”**: ì‚¬ìš©ìê°€ "ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”" ê°™ì€ ê°„ë‹¨í•œ ì¸ì‚¬ë§Œ í–ˆë‹¤ë©´, Contextë‚˜ Historyë¥¼ ë¬´ì‹œí•˜ê³  ìœ„ [ìƒí™©ë³„ ëŒ€ì‘ - ì¸ì‚¬]ì˜ ì˜ˆì‹œì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ í™˜ì˜í•˜ê³  ì•ˆì „ê°ì„ ì£¼ë©´ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.
"""

# -------------------------------------------------------------
# Retrieval with Distance Score Display
# -------------------------------------------------------------

def retrieve_with_scores(
    query: str,
    vector_store: Optional[VectorStore] = None,
    n_results: int = 5
) -> List[Dict]:
    """
    VectorStoreì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  distance ì ìˆ˜ë¥¼ í„°ë¯¸ë„ì— ì¶œë ¥
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
        n_results: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
    
    Returns:
        ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (content, metadata, distance í¬í•¨)
    """
    if vector_store is None:
        print("[Retrieval] VectorStore ì—†ìŒ - Mock ë¬¸ì„œ ì‚¬ìš©")
        return []
    
    try:
        # VectorStoreì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        search_results = vector_store.search(
            query=query,
            n_results=n_results
        )
        
        # ê²°ê³¼ íŒŒì‹± ë° distance ì¶œë ¥
        docs = []
        ids = search_results.get("ids", [[]])[0]
        documents = search_results.get("documents", [[]])[0]
        metadatas = search_results.get("metadatas", [[]])[0]
        distances = search_results.get("distances", [[]])[0]
        
        print(f"\n[Retrieval] ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ")
        for i, (doc_id, doc, meta, dist) in enumerate(zip(ids, documents, metadatas, distances)):
            print(f"  Doc {i+1} | Distance: {dist:.4f} | ID: {doc_id[:30]}...")
            docs.append({
                "content": doc,
                "metadata": meta,
                "distance": dist,
                "id": doc_id
            })
        print()
        
        return docs
    
    except Exception as e:
        print(f"[Retrieval Error] {e}")
        return []


# -------------------------------------------------------------
# Emotion Analysis
# -------------------------------------------------------------

EMOTION_ANALYSIS_PROMPT = """\
ë‹¹ì‹ ì€ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°ì • ìƒíƒœë¥¼ ì ìˆ˜ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

ë‹¤ìŒ ê°ì • ì°¨ì›ì„ 0-10ì  ì²™ë„ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
- ìš°ìš¸ê° (Depression): 0=ì—†ìŒ, 10=ë§¤ìš° ì‹¬ê°
- ë¶ˆì•ˆê° (Anxiety): 0=ì—†ìŒ, 10=ë§¤ìš° ì‹¬ê°
- ë¶„ë…¸/ì§œì¦ (Anger): 0=ì—†ìŒ, 10=ë§¤ìš° ì‹¬ê°
- ìŠ¤íŠ¸ë ˆìŠ¤ (Stress): 0=ì—†ìŒ, 10=ë§¤ìš° ì‹¬ê°
- ê¸ì • ê°ì • (Positive): 0=ì—†ìŒ, 10=ë§¤ìš° ë†’ìŒ

ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ìš©:
{conversation}

ìœ„ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ **ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ** ë‹µë³€í•˜ì„¸ìš”:
ìš°ìš¸ê°: [ì ìˆ˜]
ë¶ˆì•ˆê°: [ì ìˆ˜]
ë¶„ë…¸/ì§œì¦: [ì ìˆ˜]
ìŠ¤íŠ¸ë ˆìŠ¤: [ì ìˆ˜]
ê¸ì • ê°ì •: [ì ìˆ˜]

ê°„ë‹¨ ë¶„ì„: [1-2ë¬¸ì¥ìœ¼ë¡œ í˜„ì¬ ê°ì • ìƒíƒœ ìš”ì•½]
"""

def analyze_emotion_scores(history: List[Dict], model) -> str:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì • ì ìˆ˜ ë°˜í™˜
    
    Args:
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬
        model: LLM ëª¨ë¸
    
    Returns:
        ê°ì • ì ìˆ˜ ë¶„ì„ ê²°ê³¼ ë¬¸ìì—´
    """
    if not history:
        return "ë¶„ì„í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    
    # ìµœê·¼ ëŒ€í™” ë‚´ìš©ë§Œ ì‚¬ìš© (ìµœëŒ€ 10ê°œ)
    recent_history = history[-10:] if len(history) > 10 else history
    conversation_text = format_history(recent_history)
    
    try:
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            ("user", EMOTION_ANALYSIS_PROMPT)
        ])
        
        chain = prompt | model | StrOutputParser()
        result = chain.invoke({"conversation": conversation_text})
        
        return f"\n\nğŸ“Š **ê°ì • ìƒíƒœ ë¶„ì„**\n{result.strip()}"
    
    except Exception as e:
        return f"\n\n[Error] ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# -------------------------------------------------------------
# answer helper functions
# -------------------------------------------------------------

def format_sources(docs: List[Dict]) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì…ë ¥í•˜ê¸° ì¢‹ì€ ë¬¸ìì—´ í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (content, metadata ë“± í¬í•¨)
    
    Returns:
        Formatted context string
    """
    if not docs:
        return "ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‹¤ì œ contentê°€ ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
    valid_docs = [doc for doc in docs if doc.get("content", "").strip()]
    
    if not valid_docs:
        return "ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

    formatted_docs = []
    for i, doc in enumerate(valid_docs):
        content = doc.get("content", "").strip()
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œ VectorDB í‚¤ ì‚¬ìš©)
        metadata = doc.get("metadata", {})
        session_id = metadata.get("session_id", "")
        category = metadata.get("default_category") or metadata.get("category", "")
        
        # ê°„ê²°í•œ í¬ë§·: í•µì‹¬ ë‚´ìš©ë§Œ ì „ë‹¬
        if category:
            doc_str = f"[ìƒë‹´ì‚¬ë¡€ {i+1} - {category}]\n{content}"
        else:
            doc_str = f"[ìƒë‹´ì‚¬ë¡€ {i+1}]\n{content}"
        formatted_docs.append(doc_str)
        
    return "\n\n---\n\n".join(formatted_docs)

def format_history(history: List[Dict]) -> str:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•œë‹¤.
    """
    if not history:
        return ""
    
    formatted = []
    for msg in history:
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        # role í‘œì‹œ: user -> ì‚¬ìš©ì, assistant -> ìƒë‹´ì‚¬ (í”„ë¡¬í”„íŠ¸ í†¤ì•¤ë§¤ë„ˆì— ë§ì¶¤)
        display_role = "ì‚¬ìš©ì" if role == "user" else "ìƒë‹´ì‚¬"
        formatted.append(f"{display_role}: {content}")
        
    return "\n".join(formatted)


# -------------------------------------------------------------
# LCEL Chain Factory
# -------------------------------------------------------------

def create_answer_chain(model):
    """
    LCEL ë°©ì‹ì˜ Answer Chain ìƒì„±
    Chain: Prompt | Model | StrOutputParser
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("user", """\
[ê²€ìƒ‰ëœ ë¬¸ì„œ(Context)]
{context}

[ì´ì „ ëŒ€í™”(History)]
{history}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ê³ , ë§Œì•½ ì‚¬ìš©ìì˜ ìì‚´ ìœ„í—˜ì´ ë†’ê±°ë‚˜ ì „ë¬¸ì ì¸ ìƒë‹´ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ë‹µë³€ ëì— "[EXPERT_REFERRAL_NEEDED]" íƒœê·¸ë¥¼ ë¶™ì—¬ì¤˜.
""")
    ])
    
    chain = prompt | model | StrOutputParser()
    return chain

# -------------------------------------------------------------
# Main Generate Function (Legacy Wrapper)
# -------------------------------------------------------------

def generate_answer(
    docs: List[Dict],
    query: str,
    history: Optional[List[Dict]] = None,
    session_id: Optional[int] = None,
    db: Optional[Any] = None,
    model=None,
    vector_store: Optional[VectorStore] = None,
    use_retrieval: bool = False
) -> str:
    """
    ì •í•´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ë”°ë¼ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    (LCEL create_answer_chainì„ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë˜í¼ í•¨ìˆ˜)
    
    íŠ¹ìˆ˜ í‚¤ì›Œë“œ:
    - "ê°ì •ì ìˆ˜", "ê°ì •ë¶„ì„", "ê°ì •ìƒíƒœ" ë“± -> ê°ì • ì ìˆ˜ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    
    Args:
        use_retrieval: Trueë©´ VectorStoreì—ì„œ ì‹¤ì‹œê°„ ê²€ìƒ‰ ìˆ˜í–‰ (distance ì ìˆ˜ ì¶œë ¥)
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤ (use_retrieval=Trueì¼ ë•Œ í•„ìš”)
    """
    # 1) model ì¤€ë¹„
    if model is None:
        try:
            model = create_chat_model()
        except Exception:
            return "[Error] ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (model_config.py í™•ì¸ í•„ìš”)"
    
    # 2) ê°ì • ì ìˆ˜ ìš”ì²­ ê°ì§€ (ë„ì–´ì“°ê¸° ì œê±°í•˜ê³  ê²€ì‚¬)
    query_normalized = query.lower().replace(" ", "")
    
    # í‚¤ì›Œë“œ ê·¸ë£¹: ê°ì •/ê¸°ë¶„ + ì ìˆ˜/ë¶„ì„/ìƒíƒœ
    emotion_words = ["ê°ì •", "ê¸°ë¶„", "ë§ˆìŒ", "ì‹¬ë¦¬"]
    score_words = ["ì ìˆ˜", "ë¶„ì„", "ìƒíƒœ", "í‰ê°€", "ì¸¡ì •"]
    
    # ê°ì • ê´€ë ¨ ë‹¨ì–´ì™€ ì ìˆ˜/ë¶„ì„ ë‹¨ì–´ê°€ í•¨ê»˜ ìˆìœ¼ë©´ ê°ì • ë¶„ì„ ì‹¤í–‰
    has_emotion = any(word in query_normalized for word in emotion_words)
    has_score = any(word in query_normalized for word in score_words)
    
    if has_emotion and has_score:
        # ê°ì • ë¶„ì„ ì‹¤í–‰
        emotion_result = analyze_emotion_scores(history if history else [], model)
        return f"ë„¤, ìµœê·¼ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•´ë“œë¦´ê²Œìš”.{emotion_result}"

    # 3) Context êµ¬ì„± - VectorStoreì—ì„œ ì‹¤ì‹œê°„ ê²€ìƒ‰ ë˜ëŠ” ê¸°ì¡´ docs ì‚¬ìš©
    if use_retrieval and vector_store:
        retrieved_docs = retrieve_with_scores(query, vector_store, n_results=5)
        context_text = format_sources(retrieved_docs)
    else:
        context_text = format_sources(docs)
    
    # 4) History êµ¬ì„±
    history_text = format_history(history) if history else "ì—†ìŒ"

    # 5) LCEL ì‹¤í–‰
    try:
        chain = create_answer_chain(model)
        answer = chain.invoke({
            "context": context_text,
            "history": history_text,
            "query": query
        })
        answer = answer.strip()
        
        # 6) ì „ë¬¸ê°€ ì—°ê²° íŠ¸ë¦¬ê±° í™•ì¸
        if "[EXPERT_REFERRAL_NEEDED]" in answer:
            # íƒœê·¸ ì œê±°
            answer = answer.replace("[EXPERT_REFERRAL_NEEDED]", "").strip()
            
            # DBì— ê¸°ë¡
            if db and session_id:
                try:
                    db.create_expert_referral(
                        session_id=session_id,
                        severity_level="high", # LLM íŒë‹¨ ê¸°ë°˜ì´ë¯€ë¡œ ì¼ë‹¨ highë¡œ ì„¤ì •í•˜ê±°ë‚˜ ë³„ë„ ë¡œì§ í•„ìš”
                        recommended_action="ì „ë¬¸ ìƒë‹´ì‚¬ ì—°ê²° ê¶Œì¥"
                    )
                    # ì•ˆë‚´ ë©˜íŠ¸ ì¶”ê°€ (ì´ë¯¸ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë‚˜ í™•ì‹¤íˆ í•˜ê¸° ìœ„í•´)
                    referral_msg = "\n\n(ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì´ í•„ìš”í•´ ë³´ì—¬ ì „ë¬¸ ìƒë‹´ ì„¼í„° ì •ë³´ë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.)"
                    if "ìƒë‹´" not in answer:
                         answer += referral_msg
                except Exception as e:
                    print(f"[Error] Expert referral logging failed: {e}")

        return answer
        
    except Exception as e:
        return f"[Error] ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# -------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------

if __name__ == "__main__":
    # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
    print("=== RAG Answer Generation Test ===")
    print("'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ\n")
    
    # VectorStore ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
    use_vector = input("VectorStore ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸: n): ").strip().lower()
    use_retrieval = use_vector == 'y'
    
    vector_store = None
    if use_retrieval:
        try:
            print("\n[ì´ˆê¸°í™”] VectorStore ë¡œë”© ì¤‘...")
            vector_store = VectorStore()
            doc_count = vector_store.get_document_count()
            print(f"[ì´ˆê¸°í™”] VectorStore ì¤€ë¹„ ì™„ë£Œ - {doc_count:,}ê°œ ë¬¸ì„œ\n")
        except Exception as e:
            print(f"[ì´ˆê¸°í™” ì‹¤íŒ¨] VectorStoreë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("[ì´ˆê¸°í™”] Mock ë¬¸ì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")
            use_retrieval = False
    
    # Mock Docs (VectorStore ë¯¸ì‚¬ìš© ì‹œ)
    mock_docs = [
        {"content": "ìš°ìš¸ì¦ì€ ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ìœ¼ë©´ í˜¸ì „ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "metadata": {"category": "DEPRESSION", "speaker": "ìƒë‹´ì‚¬", "severity": 2}},
        {"content": "ê·œì¹™ì ì¸ ìš´ë™ê³¼ ìˆ˜ë©´ì´ ì •ì‹  ê±´ê°•ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.", "metadata": {"category": "NORMAL", "speaker": "ìƒë‹´ì‚¬", "severity": 0}}
    ]
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬
    history = []
    
    try:
        model = create_chat_model()
        
        while True:
            # ì‚¬ìš©ì ì…ë ¥
            query = input("\n[ë‹¹ì‹ ] ").strip()
            
            if query.lower() in ["exit", "ì¢…ë£Œ", "quit"]:
                print("í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not query:
                continue
            
            # generate_answer í•¨ìˆ˜ ì‚¬ìš© (í‚¤ì›Œë“œ ê°ì§€ í¬í•¨)
            response = generate_answer(
                docs=mock_docs,
                query=query,
                history=history,
                model=model,
                vector_store=vector_store,
                use_retrieval=use_retrieval
            )
            
            print(f"\n[ìƒë‹´ì‚¬] {response}")
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            history.append({"role": "user", "content": query})
            history.append({"role": "assistant", "content": response})
            
    except KeyboardInterrupt:
        print("\n\ní…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"[Error] {e}")
        print(f"[Error] {e}")